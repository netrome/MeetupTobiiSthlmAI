# 1. Polite phrase
Good evening everyone, and welcome to my presentation on GANs.

# 2. Respect phrase
I am honored to be here tonigt to give you this talk. I know many of you have not been here at Tobii before and I appreciate that you took the time and traveled here to hear about unsupervised learning.

# 3. Attention grabber

Now, have any one of you ever had a heated argument with your friends?

Imagine you're at a bar with your friends discussing generative models. They are working on a project, but none of their algorithms work nearly as good as they would hope. You suddenly get a crazy yet simple idea that you are convinced will work, so you try to persuade them to do this. But your friends do not beleive in your idea and argues that it is doomed to fail. 

What would you do when you get home, realize that it is late and go to sleep or spend the rest of the night coding your idea to prove your friends wrong? **-Pause-** -Obviously attempting to prove your friends wrong right?

This was the situation Ian Goodfellow was in four years ago. He found out the very same night that his idea worked. Now, four years later we know that what he did that night drastically changed the landscape of unsupervised learning.

# 4. Subject/agenda
During this talk we will learn about GANs by creating one from scratch. Then I will go through some pitfalls to avoid when working with GANs. Finally I will show you some images of eyes generated by my own GAN variant.

# 5. Objectives (why)
The objectives of this presentation is to give you an understanding of:

- How GANs work

- Why training GANs differ from training a normal DNN

- Show examples of GANs in action

# 6. Build Ethos
I have learned a lot about GANs in the last year.

I am an Algorithm Engineer at Tobii. I have recently finished the Machine Learning Master's Programme at KTH where I wrote my thesis on GANs, and I find these models super weird, horrible and amazing at the same time. But don't just take my word for it,

Yann LeCun says GANs are: “the coolest idea in deep learning in the last 20 years.”

And it is not just Yann LeCun. All the big companies, Facebook, Google, Apple, NVIDIA etc. are publishing loads of research on GANs. At Tobii we are especially interested since both Google and Apple use GANs for gaze estimation, which is our core business. You can imagine that wee need to be on our toes.

# 7. Practical info
Before we dig in, let's take some practical info. The presentation is around X minutes. There will be time reserved for questions before I summarize the presentation, but please don't hesitate to ask any questions as we go.

------------------------ Main --------------------
# Content #1: Building a GAN
So how do GANs work? What is the key idea?

## Slide: adversarial game pic (Artist and critic, pasted G and D on them)
-- Explain how G and D learn by analogy --

To fully understand how this works let's translate this to code.

First define G and D as neural networks **network definitions appear**

In pytorch we train these with optimizers **optimizers appear**

G needs some input to generate the images **latent point**

We also need some utilities before we create the training loop **vis and dataloader apperas**

Now we're ready to define the loop **loop boilerplate**

The training loop alternates between updating the generator **generator update** and discriminator **discriminator update**

The key here is that G and D have different objectives, and both are dependent on the other one. As you can see, D is trained with the normal corss entropy loss (ignoring a factor of 0.5). G on the other hand is trained with the negative log likelihood on the fake predictions to maximize the prediction score that D gives on fake images. When D is strong enough, the only way for G to minimize its objective is to generate images that are indistinguishable from the original data. Let's see this in action

## Live training of GAN

# Content #2: What could possibly go wrong?
- Batch normalization

- Mode collapse

- Non-convergence

- Vanishing gradients

- Exploding gradients

- Sparse gradients

- Imbalanced networks

- Repeating update loop

- Momentum in optimizer (The parameters I've shown are bad)

# Content #3: My GAN

Combination of autoencoder and GAN, exists many variants but this constellation was sufficient for my purposes.

## Bild på konstellation

------------------------ End ---------------------
# 1. Questions
Before I summarize, do we have any questions?

One last question.
# 2. Summary
Great, so we have gone through the code to train a GAN, we have seen how it beahves on MNIST. We have also gone through what to look out for when working with GANs, and I have shown how I tackled these problems in my project.

# 3. Confirm objectives
The objectives of this presentation was to explain how GANs work. Also rather than focusing on what is the state of the art right now another objective was to warn you about common problems with these models. I hope you feel that what you have gained today is both interesting and will be helpful to you in the future.

# 4. Final (limbic uppercut)
If nothing else, the story of GANs is a great example of what can be accomplished if you dare to listen to your gut when your friends say you're wrong.
## Picture of disagreeing people

